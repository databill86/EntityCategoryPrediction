{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_word(word_length):\n",
    "    word_len = np.random.randint(*word_length)\n",
    "    return ''.join(random.sample(string.ascii_lowercase, word_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_size = 6\n",
    "num_topics = 20\n",
    "\n",
    "topics = [\n",
    "    [gen_word((2,6)) for _ in range(topic_size)] for _ in range(num_topics)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['srpq', 'mfvt', 'oicel', 'so', 'ov', 'rmg'],\n",
       " ['hs', 'cjhrt', 'gr', 'nbtz', 'vnbf', 'pjn'],\n",
       " ['migw', 'gx', 'tyhfl', 'vfim', 'qtd', 'es'],\n",
       " ['gfsrc', 'dvr', 'xyn', 'nzr', 'ks', 'wnk'],\n",
       " ['khvd', 'sd', 'xh', 'xurpb', 'tbzh', 'xpfh'],\n",
       " ['akqw', 'ybov', 'bacj', 'vawin', 'lirjt', 'yigk'],\n",
       " ['kmbd', 'lvhm', 'wzr', 'em', 'typm', 'cxj'],\n",
       " ['abzxw', 'qeg', 'zsl', 'ra', 'aowf', 'rpb'],\n",
       " ['arg', 'em', 'kic', 'by', 'gin', 'ovq'],\n",
       " ['fuwyi', 'av', 'hmb', 'eurjf', 'bur', 'ad'],\n",
       " ['kwejo', 'ku', 'pgns', 'ztu', 'snm', 'edpmj'],\n",
       " ['uoyxf', 'hgr', 'xg', 'swb', 'emns', 'hd'],\n",
       " ['bvl', 'eh', 'kb', 'bf', 'rn', 'rnlg'],\n",
       " ['dwbn', 'atds', 'dklyp', 'pmj', 'arsd', 'tcvma'],\n",
       " ['ufp', 'omh', 'ex', 'twygo', 'rh', 'vabxq'],\n",
       " ['cenzb', 'ak', 'lps', 'hkgm', 'npkji', 'ozv'],\n",
       " ['ozhkl', 'ib', 'zxtrg', 'qkpg', 'yw', 'rp'],\n",
       " ['nr', 'fdbv', 'djt', 'jcfvh', 'lumt', 'bs'],\n",
       " ['qs', 'ukhbn', 'zxy', 'abgc', 'omlt', 'yajgk'],\n",
       " ['mv', 'kytnc', 'bjfs', 'cfs', 'dqmec', 'opkbh']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(m_topics):\n",
    "    sent = []\n",
    "    prefix_topics = 1\n",
    "    suffix_topics = 1\n",
    "    \n",
    "    for i in range(prefix_topics):\n",
    "        sent += random.sample(topics[random.choice(range(num_topics))], random.randint(0,2))\n",
    "    \n",
    "    mention = random.choice(random.choice(topics))\n",
    "    \n",
    "    sent += random.sample(topics[random.choice(m_topics)], random.randint(0,2)) + [\"\\t\", mention, \"\\t\"] + random.sample(topics[random.choice(m_topics)], random.randint(0,2))\n",
    "    \n",
    "    for i in range(suffix_topics):\n",
    "        sent += random.sample(topics[random.choice(range(num_topics))], random.randint(0,2))\n",
    "      \n",
    "    return sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([4, 12], ['khvd', '\\t', 'oicel', '\\t', 'swb'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_topics = random.choices(range(num_topics), k=2)\n",
    "m_topics, generate_sentence(m_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_to_file(file_path, count=1000):\n",
    "    with open(file_path, 'w') as out:\n",
    "        for i in range(count):\n",
    "            m_topics = random.choices(range(num_topics), k=2)\n",
    "            topics_str = \"_\".join(map(str, m_topics))\n",
    "            for j in range(random.randint(2, 4)):\n",
    "                sent = generate_sentence(m_topics)\n",
    "                out.write(topics_str + \"\\t\" + ' '.join(sent) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_to_file('../data/fake_data_train.tsv', count=10000)\n",
    "generate_to_file('../data/fake_data_valid.tsv', count=500)          \n",
    "generate_to_file('../data/fake_data_test.tsv', count=500) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/fake_ft_data.tsv', 'w') as out:\n",
    "    for i in range(1000):\n",
    "        m_topics = random.choices(range(num_topics), k=2)\n",
    "        topics_str = \"_\".join(map(str, m_topics))\n",
    "        for j in range(random.randint(2, 4)):\n",
    "            sent = generate_sentence(m_topics)\n",
    "            out.write(' '.join(sent) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install git+https://github.com/facebookresearch/fastText.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fastText.train_unsupervised(\n",
    "    input='../data/fake_ft_data.tsv', minCount=0, bucket=1000, dim=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.get_words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(\"../data/fake_ft_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from gensim.models.utils_any2vec import ft_ngram_hashes\n",
    "from gensim.models.utils_any2vec import compute_ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/generall/sources/Sci/ml/allen_evn/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `load_fasttext_format` (use load_facebook_vectors (to use pretrained embeddings) or load_facebook_model (to continue training with the loaded full model, more RAM) instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "ft = FastText.load_fasttext_format(\"../data/fake_ft_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft.save('../data/gensim_fake_ft.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ft2 = FastText.load('../data/gensim_fake_ft.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/generall/sources/Sci/ml/allen_evn/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `num_ngram_vectors` (Attribute will be removed in 4.0.0, use self.wv.num_ngram_vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ft2.num_ngram_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_mapping = {}\n",
    "for idx, _ in enumerate(topics):\n",
    "    for idx2, _ in enumerate(topics):\n",
    "        topics_mapping[f\"{idx}_{idx2}\"] = [str(idx), str(idx2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/category_mapping.json', 'w') as out:\n",
    "    json.dump(topics_mapping, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating file '../data/fake_data_train.tsv_aa'\n",
      "creating file '../data/fake_data_train.tsv_ab'\n",
      "creating file '../data/fake_data_train.tsv_ac'\n",
      "creating file '../data/fake_data_train.tsv_ad'\n",
      "creating file '../data/fake_data_train.tsv_ae'\n",
      "creating file '../data/fake_data_train.tsv_af'\n",
      "creating file '../data/fake_data_train.tsv_ag'\n",
      "creating file '../data/fake_data_train.tsv_ah'\n",
      "creating file '../data/fake_data_train.tsv_ai'\n",
      "creating file '../data/fake_data_train.tsv_aj'\n"
     ]
    }
   ],
   "source": [
    "!split -n l/10 --verbose ../data/fake_data_train.tsv ../data/fake_data_train.tsv_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: split [OPTION]... [FILE [PREFIX]]\n",
      "Output pieces of FILE to PREFIXaa, PREFIXab, ...;\n",
      "default size is 1000 lines, and default PREFIX is 'x'.\n",
      "\n",
      "With no FILE, or when FILE is -, read standard input.\n",
      "\n",
      "Mandatory arguments to long options are mandatory for short options too.\n",
      "  -a, --suffix-length=N   generate suffixes of length N (default 2)\n",
      "      --additional-suffix=SUFFIX  append an additional SUFFIX to file names\n",
      "  -b, --bytes=SIZE        put SIZE bytes per output file\n",
      "  -C, --line-bytes=SIZE   put at most SIZE bytes of records per output file\n",
      "  -d                      use numeric suffixes starting at 0, not alphabetic\n",
      "      --numeric-suffixes[=FROM]  same as -d, but allow setting the start value\n",
      "  -x                      use hex suffixes starting at 0, not alphabetic\n",
      "      --hex-suffixes[=FROM]  same as -x, but allow setting the start value\n",
      "  -e, --elide-empty-files  do not generate empty output files with '-n'\n",
      "      --filter=COMMAND    write to shell COMMAND; file name is $FILE\n",
      "  -l, --lines=NUMBER      put NUMBER lines/records per output file\n",
      "  -n, --number=CHUNKS     generate CHUNKS output files; see explanation below\n",
      "  -t, --separator=SEP     use SEP instead of newline as the record separator;\n",
      "                            '\\0' (zero) specifies the NUL character\n",
      "  -u, --unbuffered        immediately copy input to output with '-n r/...'\n",
      "      --verbose           print a diagnostic just before each\n",
      "                            output file is opened\n",
      "      --help     display this help and exit\n",
      "      --version  output version information and exit\n",
      "\n",
      "The SIZE argument is an integer and optional unit (example: 10K is 10*1024).\n",
      "Units are K,M,G,T,P,E,Z,Y (powers of 1024) or KB,MB,... (powers of 1000).\n",
      "\n",
      "CHUNKS may be:\n",
      "  N       split into N files based on size of input\n",
      "  K/N     output Kth of N to stdout\n",
      "  l/N     split into N files without splitting lines/records\n",
      "  l/K/N   output Kth of N to stdout without splitting lines/records\n",
      "  r/N     like 'l' but use round robin distribution\n",
      "  r/K/N   likewise but only output Kth of N to stdout\n",
      "\n",
      "GNU coreutils online help: <http://www.gnu.org/software/coreutils/>\n",
      "Full documentation at: <http://www.gnu.org/software/coreutils/split>\n",
      "or available locally via: info '(coreutils) split invocation'\n"
     ]
    }
   ],
   "source": [
    "!split --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
